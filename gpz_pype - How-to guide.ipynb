{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91284771",
   "metadata": {},
   "source": [
    "# GPz_Pype: How-to for the basic features and usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4928d64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d9c3e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Plotting modules\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi']= 100\n",
    "\n",
    "# Astropy modules\n",
    "from astropy.table import Table, join, vstack, hstack\n",
    "from astropy.coordinates import SkyCoord\n",
    "import astropy.units as u\n",
    "\n",
    "# Import required gpz_pype functions\n",
    "from gpz_pype.utilities import Params, set_gpz_path, basic_lupt_soft, flux_to_lupt\n",
    "from gpz_pype.base import GPz\n",
    "from gpz_pype.gmm import GMMbasic\n",
    "\n",
    "# # Other modules - might not be required\n",
    "# import subprocess\n",
    "# import pickle\n",
    "\n",
    "def calcStats(photoz, specz):\n",
    "    cut = np.logical_and(photoz >= 0, specz >= 0.)\n",
    "    #print('NGD: {0}'.format(cut.sum()))\n",
    "    dz = photoz - specz\n",
    "    abs_dz = np.abs(dz)/(1+specz)\n",
    "\n",
    "    p90 = (abs_dz < np.percentile(abs_dz, 90.))\n",
    "    sigma_90 = np.sqrt(np.sum((dz[p90]/(1+specz[p90]))**2) / float(len(dz)))\n",
    "\n",
    "    bias = np.nanmedian(dz[cut]/(1+specz[cut]))\n",
    "    ol1 = (abs_dz > 0.15)\n",
    "    nmad = 1.48 * np.median( np.abs(dz[cut] - np.median(dz[cut])) / (1+specz[cut]))\n",
    "    ol2 = (abs_dz > (3*nmad))\n",
    "    OLF1 = np.sum( ol1[cut] ) / float(len(dz[cut]))\n",
    "    OLF2 = np.sum( ol2[cut] ) / float(len(dz[cut]))\n",
    "    \n",
    "    ol1_s, ol2_s = np.invert(ol1), np.invert(ol2)\n",
    "\n",
    "    return nmad, sigma_90, OLF1, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f0a8d6",
   "metadata": {},
   "source": [
    "For this example notebook we will use the CANDELS COSMOS catalogue as an input. This includes lots of photometry from both ground + space, but we will restrict analysis to just the HST filters for simplicity (and to mimic the likely limitations of some JWST fields).\n",
    "\n",
    "Inputting the catalogue into GPz however requires some additional pre-processing. We start by extracting the general common columns and setting any missing `z_spec` values to `np.nan`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f13cf091",
   "metadata": {},
   "outputs": [],
   "source": [
    "candels = Table.read('MPhys/CANDELS/CANDELS.COSMOS.F160W.Processed.photz.fits', format='fits')\n",
    "\n",
    "catalogue = candels[['ID', 'RA', 'DEC', 'CLASS_STAR', 'FLAGS', 'EBV', 'z_spec']]\n",
    "\n",
    "catalogue['z_spec'][catalogue['z_spec'] < 0] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6352f81b",
   "metadata": {},
   "source": [
    "To further trim the catalogue down, i next want to pull out only the flux/magnitude columns that belong to HST filters. In this case, columns that start with either 'ACS' or 'WFC'. As with `z_spec`, any value set as -99 in the input catalogue (i.e. no data) is converted to `np.nan`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45d9360c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in candels.colnames:\n",
    "    if col.startswith('ACS') or col.startswith('WFC'):\n",
    "        inst, filt, ext = col.split('_') # Split column name into 3 parts based on '_'\n",
    "        col_vals = candels[col]\n",
    "        col_vals[col_vals < -90] = np.nan\n",
    "        catalogue[f'{ext}_{filt}'] = col_vals # [FLUX/FLUXERR]_[FILTER]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7875bac8",
   "metadata": {},
   "source": [
    "To extract the filters that were found in the catalogue, we reprocess the column names to look for filters. This could have been done above, but since there were both MAG/FLUX columns this would have resulted in duplicates. So this is simpler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f4d86e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['F606W', 'F814W', 'F125W', 'F160W']\n"
     ]
    }
   ],
   "source": [
    "filters = [col.split('_')[1] for col in catalogue.colnames if col.startswith('FLUX_')]\n",
    "print(filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca6a603",
   "metadata": {},
   "source": [
    "### Asinh magnitudes\n",
    "Extensive past experience has shown that training machine learning using magnitudes can provide significantly better results than using linear flux values. This is particularly true for fields with very high dynamic range, such as HST/JWST deep fields with extremely sensitive observations.\n",
    "\n",
    "However, the significant drawback of normal AB magnitudes is that they cannot be used at low S/N or for negative fluxes (consistent with zero). For GPz we therefore make use of asinh magnitudes, which remain real+positive for very low S/N that remain informative for non-detections without the need for additional missing value prediction etc. This is particularly key for high-redshift where the non-detection at bluer wavelengths is critical for the redshift estimate.\n",
    "\n",
    "Ideally, the softening parameter ($b$) used to derive asinh magnitudes from fluxes + uncertainties will be derived from the local noise on a per-object basis. However, when these are not available the use of a global softening parameter does not significantly impact photo-z results. \n",
    "\n",
    "For convenience, `gpz_pype` includes a function for estimating a suitable global softening parameter based on a set of input fluxes+errors (_that are assumed to be representative of the full field_):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5b67c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate a softening parameter for each filter in the list of filters derived above:\n",
    "b_arr = [basic_lupt_soft(catalogue[f'FLUX_{filt}'], catalogue[f'FLUXERR_{filt}']) for filt in filters] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa5d8f8",
   "metadata": {},
   "source": [
    "With softening parameters calculated, we can then calculate the asinh magnitudes (also known as 'luptitudes') for each of our filters. These can be calculated using the `flux_to_lupt` function included in `gpz_pype`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21e3465f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 0. 0. ... 3. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "# Make a new catalogue with the relevant key reference columns:\n",
    "lupt_cols = catalogue[['ID', 'RA', 'DEC', 'CLASS_STAR', 'FLAGS', 'EBV', 'z_spec']]\n",
    "\n",
    "check_nans = np.zeros(len(catalogue)) # Running counter of null values for each object\n",
    "\n",
    "for filt, b in zip(filters, b_arr):\n",
    "    lupt, lupterr = flux_to_lupt(catalogue[f'FLUX_{filt}'], # Input flux (uJy)\n",
    "                                 catalogue[f'FLUXERR_{filt}'], # Input uncertainty (uJy)\n",
    "                                 b, # Filter specific softening parameter\n",
    "                                ) \n",
    "    \n",
    "    lupt_cols[f'lupt_{filt}'] = lupt\n",
    "    lupt_cols[f'lupterr_{filt}'] = lupterr\n",
    "    \n",
    "    check_nans += np.isnan(lupt) # Update nan counter for this filter\n",
    "print(check_nans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62676ed2",
   "metadata": {},
   "source": [
    "After filling all the columns, for the purpose of training GPz we want to cut our catalogue down to those sources with values in all the filters we want to use and for which there is a spectroscopic redshift:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e8c9d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True  True ... False False False]\n"
     ]
    }
   ],
   "source": [
    "good = (check_nans == 0) # 'Good' sources for training are those with 0 NaNs\n",
    "print(good)\n",
    "\n",
    "cat = lupt_cols[good * (np.isnan(lupt_cols['z_spec']) == False)] # Keep only good sources with z_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36257ff",
   "metadata": {},
   "source": [
    "## Running GPz through `gpz_pype`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84901c1d",
   "metadata": {},
   "source": [
    "The following code assumes that `gpz++` is correctly installed on the system and that it can be run through the command line without issue. The `gpz_pype` classes simply wrap around `gpz++` and streamline the book-keeping required when using more complicated splitting+weighting of training sample.\n",
    "\n",
    "The path to `gpz++` can be set as a system variable so this step might not be required in future. It can also be input directly into the `GPz` class below, but for safety we can use the convenience function `set_gpz_path` to set this for the current session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba28535",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_gpz_path('/Users/duncan/Astro/Photoz/gpzpp/bin/gpz++')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ebf4d4",
   "metadata": {},
   "source": [
    "The main workhorse of `gpz_pype` is the `GPz` class, it needs to be instantiated with a set of parameters that we can then update manually (and which `GPz` will automatically set for us when necessary).\n",
    "We can also define the number of CPUs we want `gpz++` to make use of during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ded96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = GPz(param_file='/Users/duncan/Astro/Photoz/gpzpp/example/gpz.param', ncpu=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9176a32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.params['REUSE_MODEL'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcd7f31",
   "metadata": {},
   "source": [
    "To perform GPz training on an input catalogue in gpz++ in our most straight-forward approach, we can use the `.run_training` function from the `GPz` class.\n",
    "\n",
    "Even in its most straight-forward approach, there are lots of options that must be set. Many of which can be left as their degault value. I have tried to ensure that in-code documentation for `gpz_pype` is relatively complete, so the full set of options available can be seen using the standard python help functionality, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86ffbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(test.run_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0bd238",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_run, paths = test.run_training(\n",
    "      cat, # Run training on this input catalogue - can be a path or a Table object\n",
    "      outdir='test_dir', # Save training/output catalogues in this directory\n",
    "      basename='candels_cosmos', # Start our output filenames with this\n",
    "      bash_script=False, # Run GPz directly, don't just write the parameter files + commands to bash\n",
    "      mag_prefix='lupt_', # Look for these asinh magnitude columns\n",
    "      error_prefix='lupterr_', # Look for these error columns\n",
    "      id_col='ID', # ID column to propagate into output files\n",
    "      total_basis_functions=100, # NUMBF passed to gpz++\n",
    "      do_iteration=False, # If True, run second iteration with more complex covariance\n",
    "      verbose=False, # Don't print all of the gpz++ output to screen. Turn on for debugging\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e74002",
   "metadata": {},
   "source": [
    "There are two outputs from `.run_training`:\n",
    "\n",
    "1. Catalogue: The 'test' subset of the input catalogue, with `gpz++` prediction columns appended:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c1f57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_run.show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd70a636",
   "metadata": {},
   "source": [
    "2. Output paths: A dictionary containing the relative paths to the various files produced from the gpz++ training - including both input catalogues (in gpz format) and the trained model + associated parameter file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f375f14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1199df",
   "metadata": {},
   "source": [
    "### Plotting outputs\n",
    "Just for validation purposes, lets plot the predicted photo-$z$ versus spec-$z$ for the test catalogue. We can see that between $1 < z < 3$ the predictions do a decent job, even with just the 4-bands used in training. But at low and high redshifts the performance falls off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16611f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function x**(1/2)\n",
    "def forward(x):\n",
    "    return np.log10(1+x)\n",
    "\n",
    "def inverse(x):\n",
    "    return (10**x) - 1\n",
    "\n",
    "Fig, Ax = plt.subplots(1,1,figsize=(4.5,4.5))\n",
    "\n",
    "Ax.errorbar(simple_run['z_spec'], simple_run['value'], \n",
    "            yerr=simple_run['uncertainty'], \n",
    "            fmt='o', ms=3, color='k', ecolor='k', alpha=0.2)\n",
    "Ax.set_xlim([0, 7])\n",
    "Ax.set_ylim([0, 7])\n",
    "\n",
    "Ax.set_yscale('function', functions=(forward, inverse))\n",
    "Ax.set_xscale('function', functions=(forward, inverse))\n",
    "\n",
    "\n",
    "Ax.plot([0, 7], [0, 7], '--', color='0.5', lw=2)\n",
    "Ax.set_xlabel(r'$z_{\\rm{spec}}$')\n",
    "Ax.set_ylabel(r'$z_{\\rm{phot}}$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb99305",
   "metadata": {},
   "source": [
    "## Example GMM Divide + Cost-sensitive Learning Calculations\n",
    "\n",
    "The basic functionality for the Gaussian Mixture Model (GMM) sample division and cost-sensitive learning is contained in the `GMMbasic` class. The training of the respective GMMs is done internally within the class, and is effectively a wrapper around the [sci-kit learn/GaussianMixture](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture) classes with all its functionality.\n",
    "\n",
    "The key inputs that we need to define for `GMMbasic` are as follows:\n",
    "\n",
    "- `X_pop` : The feature array for the reference population.\n",
    "- `X_train` : The feature array for the training sample.\n",
    "- `Y_train` : The labels for the training sample (i.e. the spectroscopic redshifts).\n",
    "- `ncomp` : Number of mixtures used for the GMM model.\n",
    "\n",
    "For building the mixture models, we need to manually decide on and create a set of features (`X_pop`/`X_train`) with which we want to represent our sample. The 'best' features will be dependent on the input data, the scientific goals etc. These also do not necessarily need to be features used in the GPz training (e.g. luptitudes), or even things derived from luptitudes. \n",
    "\n",
    "In the following example, given the limited number of features we will just use a combination of colours and magnitudes. But sizes or morphological information could also be sensible choices.\n",
    "\n",
    "Since for our example the training set is a subset of the reference population, we can define one set of features and then split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa0582c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_features = np.array([lupt_cols['lupt_F606W']-lupt_cols['lupt_F814W'],\n",
    "                         lupt_cols['lupt_F814W']-lupt_cols['lupt_F125W'],\n",
    "                         lupt_cols['lupt_F160W']]).T\n",
    "\n",
    "X_pop = gmm_features[good] # The full reference population \n",
    "                           # (i.e. representative of the full sample we would like photo-z predictions for)\n",
    "X_train = gmm_features[good * (np.isnan(lupt_cols['z_spec']) == False)] # The training subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bc0545",
   "metadata": {},
   "source": [
    "Instantiating the class with the inputs defined above will build the GMMs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ba6b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GMMbasic(X_pop=X_pop, \n",
    "               X_train=X_train, \n",
    "               Y_train=cat['z_spec'], \n",
    "               ncomp=4) # For larger samples and more features, more mixtures could be appropriate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14deabf3",
   "metadata": {},
   "source": [
    "## GMM Weight\n",
    "\n",
    "Since the cost-sensitive learning (CSL) is the most straight-forward to include in `gpz++`, we will first generate some CSL weights based on the GMMs we have just produced. To do so is as straight-forward as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11822d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = gmm.calc_weights(X_train, X_pop, max_weight=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a6e703",
   "metadata": {},
   "source": [
    "In principle, we could go straight to running GPz with these inputs. But its first useful to verify that the weights are producing sensible results. And if necessary, to quantitatively compare how well different features perform when trying to \n",
    "\n",
    "For the feature set we defined earlier, we can simply plot the distributions of the training sample features, $x_{i}$, before and after weighting is applied:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff38ef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fig, Ax = plt.subplots(1,3, figsize=(9,3))\n",
    "\n",
    "Ax = Ax.flatten()\n",
    "\n",
    "for i, ax in enumerate(Ax):\n",
    "    c, bins, _ = ax.hist(X_pop[:,i], density=True, bins=25, \n",
    "                      range=np.percentile(X_pop[:,i], [0.1, 99.9]), histtype='step', lw=2,\n",
    "                        label='All')\n",
    "    ax.hist(X_train[:,i], density=True, bins=bins, histtype='step', lw=2, color='firebrick',\n",
    "            label='Training')\n",
    "    ax.hist(X_train[:,i], density=True, bins=bins, histtype='step', lw=2, color='firebrick', \n",
    "            ls='--', weights=weights,\n",
    "            label='Weighted Training')\n",
    "\n",
    "    ax.set_ylabel('PDF(x)')\n",
    "    ax.set_xlabel(f'$x_{i+1}$')\n",
    "\n",
    "Ax[0].legend(loc='upper right', prop={'size':7})\n",
    "Fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a19113",
   "metadata": {},
   "source": [
    "Although not perfect, we can see that the fainter training sources have been significantly up-weighted when compared to the original distribution. The colours are also distributed more like those of the full sample.\n",
    "\n",
    "To include the weights in GPz we can simply add them to our input catalogue and tell GPz to include them in the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819aee6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat['weights'] = weights # Add the weights to our previous catalogue\n",
    "\n",
    "weights_run, paths = test.run_training( # All of these as above but with minor changes\n",
    "      cat, \n",
    "      outdir='test_dir', \n",
    "      basename='candels_cosmos_weighted', # Change the prefix to keep separate\n",
    "      bash_script=False,\n",
    "      mag_prefix='lupt_',\n",
    "      error_prefix='lupterr_', \n",
    "      id_col='ID',\n",
    "      total_basis_functions=100,\n",
    "      do_iteration=False, \n",
    "      verbose=False,\n",
    "      weight_col='weights', # Now weight training by this column\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d1efb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fig, Ax = plt.subplots(1,2,figsize=(9,4))\n",
    "\n",
    "Ax[0].errorbar(simple_run['z_spec'], simple_run['value'], \n",
    "            yerr=simple_run['uncertainty'], \n",
    "            fmt='o', ms=3, color='k', ecolor='k', alpha=0.2)\n",
    "\n",
    "Ax[1].errorbar(weights_run['z_spec'], weights_run['value'], \n",
    "            yerr=weights_run['uncertainty'], \n",
    "            fmt='o', ms=3, color='steelblue', ecolor='steelblue', alpha=0.4)\n",
    "\n",
    "labels = ['Simple', 'Weighted']\n",
    "\n",
    "stats_simple = calcStats(simple_run['z_spec'], simple_run['value'])\n",
    "stats_weight = calcStats(weights_run['z_spec'], weights_run['value'])\n",
    "\n",
    "stats = [stats_simple, stats_weight]\n",
    "\n",
    "for i, ax in enumerate(Ax):\n",
    "    ax.set_xlim([0, 7])\n",
    "    ax.set_ylim([0, 7])\n",
    "    \n",
    "    ax.set_yscale('function', functions=(forward, inverse))\n",
    "    ax.set_xscale('function', functions=(forward, inverse))\n",
    "    ax.plot([0, 7], [0, 7], '--', color='0.5', lw=2)\n",
    "    ax.set_xlabel(r'$z_{\\rm{spec}}$')\n",
    "    ax.set_ylabel(r'$z_{\\rm{phot}}$')\n",
    "    ax.set_title(labels[i], size=11)\n",
    "    \n",
    "    ax.text(0.1, 6.0, f'$\\sigma = $ {stats[i][0]:.3f}')\n",
    "    ax.text(0.1, 5.0, f'OLF = {stats[i][2]:.3f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c162bf",
   "metadata": {},
   "source": [
    "At face value, the weights have not resulted in an overall statistical improvement to the photo-$z$ estimates. But by eye, there is definitely some improvement at the higher redshift end. For now, we move onto the GMM-Divide option.\n",
    "\n",
    "## GMM Divide\n",
    "\n",
    "Dividing the training sample into different mixtures is as equally simple. For this example with its relatively small sample sizes, the number of training sources that could end up assigned to a mixture could be very small. We therefore will lower the threshold above which a source will be assigned to a mixture to 0.2, approximately saying that the source has at least a 20% chance of belonging to that mixture. (Note the default 0.5 effectively assigns each source to its best match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43be2dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mixtures = gmm.divide(X_train[:,:], \n",
    "                            weight=False, # Do not include CSL weights yet\n",
    "                            threshold=0.2) # Change the divide threshold\n",
    "\n",
    "cat['weights'] = 1. # Re-set the weights to be equal\n",
    "\n",
    "# Save for later\n",
    "train_mixtures.write('cosmos_mixtures_ex.csv', format='ascii.csv', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7047ae63",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_output = Table.read('cosmos_mixtures_ex.csv', format='ascii.csv')\n",
    "\n",
    "divide_run, paths = test.run_training(cat, outdir='test_dir', \n",
    "                         basename='candels_cosmos_divide',\n",
    "                         gmm_output=gmm_output,\n",
    "                         bash_script=False,\n",
    "                         weight_col='weights',\n",
    "                         mag_prefix='lupt_', \n",
    "                         error_prefix='lupterr_', id_col='ID',\n",
    "                         total_basis_functions=100, do_iteration=False, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d6b7b9",
   "metadata": {},
   "source": [
    "As above, lets compare the results between the simple and GMM divided runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148a4710",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fig, Ax = plt.subplots(1,2,figsize=(9,4))\n",
    "\n",
    "Ax[0].errorbar(simple_run['z_spec'], simple_run['value'], \n",
    "            yerr=simple_run['uncertainty'], \n",
    "            fmt='o', ms=3, color='k', ecolor='k', alpha=0.2)\n",
    "\n",
    "Ax[1].errorbar(divide_run['z_spec'], divide_run['value'], \n",
    "            yerr=divide_run['uncertainty'], \n",
    "            fmt='o', ms=3, color='steelblue', ecolor='steelblue', alpha=0.4)\n",
    "\n",
    "labels = ['No divide', 'Divide']\n",
    "\n",
    "stats_simple = calcStats(simple_run['z_spec'], simple_run['value'])\n",
    "stats_divide = calcStats(divide_run['z_spec'], divide_run['value'])\n",
    "\n",
    "stats = [stats_simple, stats_divide]\n",
    "\n",
    "for i, ax in enumerate(Ax):\n",
    "    ax.set_xlim([0, 7])\n",
    "    ax.set_ylim([0, 7])\n",
    "\n",
    "    ax.set_yscale('function', functions=(forward, inverse))\n",
    "    ax.set_xscale('function', functions=(forward, inverse))\n",
    "    ax.plot([0, 7], [0, 7], '--', color='0.5', lw=2)\n",
    "    ax.set_xlabel(r'$z_{\\rm{spec}}$')\n",
    "    ax.set_ylabel(r'$z_{\\rm{phot}}$')\n",
    "    ax.set_title(labels[i], size=11)\n",
    "    \n",
    "    ax.text(0.1, 6.0, f'$\\sigma = $ {stats[i][0]:.3f}')\n",
    "    ax.text(0.1, 5.0, f'OLF = {stats[i][2]:.3f}')   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632e1f47",
   "metadata": {},
   "source": [
    "This time we can see that overall outlier fraction and scatter are now improved. Visually, we can also see that the high-redshift end is also significantly improved. \n",
    "\n",
    "Lets now do a run using both the GMM Divide and Weights. This can be done using the divide option, but setting `weight = True` in the options. To prevent confusion, we will also now remove the weight column we added above, since the weights will now be propagated through the catalogue included as `gmm_output`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6831e035",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mixtures2 = gmm.divide(X_train[:,:], \n",
    "                            weight=True,\n",
    "                            threshold=0.2) # Do not include CSL weights yet\n",
    "\n",
    "if 'weights' in cat.colnames:\n",
    "    cat.remove_columns(['weights'])  # Weights will be taken from the GMM outputs file now instead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a51358",
   "metadata": {},
   "source": [
    "Run the gpz training as normal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285081ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_run, paths = test.run_training(cat, outdir='test_dir', \n",
    "                         basename='candels_cosmos_both',\n",
    "                         gmm_output=train_mixtures2,\n",
    "                         bash_script=False,\n",
    "                         weight_col='weights',\n",
    "                         mag_prefix='lupt_', \n",
    "                         error_prefix='lupterr_', id_col='ID',\n",
    "                         total_basis_functions=100, do_iteration=False, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e58cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fig, Ax = plt.subplots(1,2,figsize=(9,4))\n",
    "\n",
    "Ax[0].errorbar(simple_run['z_spec'], simple_run['value'], \n",
    "            yerr=simple_run['uncertainty'], \n",
    "            fmt='o', ms=3, color='k', ecolor='k', alpha=0.2)\n",
    "\n",
    "Ax[1].errorbar(combined_run['z_spec'], combined_run['value'], \n",
    "            yerr=combined_run['uncertainty'], \n",
    "            fmt='o', ms=3, color='steelblue', ecolor='steelblue', alpha=0.4)\n",
    "\n",
    "labels = ['No divide', 'Divide + Weight']\n",
    "\n",
    "stats_simple = calcStats(simple_run['z_spec'], simple_run['value'])\n",
    "stats_combined = calcStats(combined_run['z_spec'], combined_run['value'])\n",
    "\n",
    "stats = [stats_simple, stats_combined]\n",
    "\n",
    "for i, ax in enumerate(Ax):\n",
    "    ax.set_xlim([0, 7])\n",
    "    ax.set_ylim([0, 7])\n",
    "\n",
    "    ax.set_yscale('function', functions=(forward, inverse))\n",
    "    ax.set_xscale('function', functions=(forward, inverse))\n",
    "    ax.plot([0, 7], [0, 7], '--', color='0.5', lw=2)\n",
    "    ax.set_xlabel(r'$z_{\\rm{spec}}$')\n",
    "    ax.set_ylabel(r'$z_{\\rm{phot}}$')\n",
    "    ax.set_title(labels[i], size=11)\n",
    "    \n",
    "    ax.text(0.1, 6.0, f'$\\sigma = $ {stats[i][0]:.3f}')\n",
    "    ax.text(0.1, 5.0, f'OLF = {stats[i][2]:.3f}')   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56479e81",
   "metadata": {},
   "source": [
    "Compared to the Divide-only run, our combined Divide+Weights is performing slightly worse for the two metrics we have chosen, but is still an improvement over the Weight-only option. So in this case, using only the GMM-Divide would be the optimal approach. But as training sample sizes increase and additional features are added to the GPz training, this might not remain the case.\n",
    "\n",
    "Additionally, since the spectroscopic sample is obviously biased, the Weight/Divide+Weight runs might actually provide better statistics for the overall galaxy population (at the expense of the brigher sources). Assessing the statistics as a function of e.g. spectroscopic redshift or magnitude can start to make these assessments clearer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
